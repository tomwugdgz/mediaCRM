#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼ºç‰ˆæ¯æ—¥ä¿¡æ¯åˆ†æç¨‹åº
åŒ…å«æ›´è¯¦ç»†çš„æ•°æ®åˆ†æåŠŸèƒ½å’Œäº¤äº’å¼ç•Œé¢
"""

import os
import re
import json
import pandas as pd
from collections import Counter, defaultdict
from datetime import datetime, timedelta
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
import jieba
from bs4 import BeautifulSoup
import requests
import streamlit as st
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots

class EnhancedInfoAnalyzer:
    def __init__(self, links_file='links.txt', articles_dir='æ–‡ç« '):
        """
        åˆå§‹åŒ–å¢å¼ºç‰ˆåˆ†æå™¨
        
        Args:
            links_file: é“¾æ¥æ–‡ä»¶è·¯å¾„
            articles_dir: æ–‡ç« å­˜å‚¨ç›®å½•
        """
        self.links_file = links_file
        self.articles_dir = articles_dir
        self.links_data = []
        self.articles_data = []
        self.df_links = None
        self.df_articles = None
        
    def load_and_process_data(self):
        """åŠ è½½å¹¶å¤„ç†æ‰€æœ‰æ•°æ®"""
        try:
            # åŠ è½½é“¾æ¥æ•°æ®
            self.load_links_data()
            
            # åŠ è½½æ–‡ç« æ•°æ®
            self.load_articles_data()
            
            # åˆ›å»ºDataFrameä¾¿äºåˆ†æ
            self.create_dataframes()
            
            return True
            
        except Exception as e:
            print(f"æ•°æ®åŠ è½½å¤±è´¥ï¼š{e}")
            return False
    
    def load_links_data(self):
        """åŠ è½½é“¾æ¥æ•°æ®"""
        with open(self.links_file, 'r', encoding='utf-8') as f:
            links = f.readlines()
        
        self.links_data = []
        for link in links:
            link = link.strip()
            if link:
                article_id = self.extract_article_id(link)
                date_info = self.extract_date_from_link(link)
                
                self.links_data.append({
                    'url': link,
                    'article_id': article_id,
                    'date': date_info,
                    'year': date_info.year if date_info else None,
                    'month': date_info.strftime('%Y-%m') if date_info else None
                })
    
    def load_articles_data(self):
        """åŠ è½½æ–‡ç« æ•°æ®"""
        self.articles_data = []
        
        if not os.path.exists(self.articles_dir):
            return
        
        for file in os.listdir(self.articles_dir):
            if file.endswith('.docx'):
                file_path = os.path.join(self.articles_dir, file)
                
                # æå–æ ‡é¢˜ä¿¡æ¯
                title = file.replace('.docx', '')
                
                # è·å–æ–‡ä»¶ä¿¡æ¯
                file_stat = os.stat(file_path)
                file_size = file_stat.st_size
                create_time = datetime.fromtimestamp(file_stat.st_ctime)
                modify_time = datetime.fromtimestamp(file_stat.st_mtime)
                
                self.articles_data.append({
                    'title': title,
                    'filename': file,
                    'filepath': file_path,
                    'file_size': file_size,
                    'create_time': create_time,
                    'modify_time': modify_time
                })
    
    def create_dataframes(self):
        """åˆ›å»ºæ•°æ®åˆ†æç”¨çš„DataFrame"""
        self.df_links = pd.DataFrame(self.links_data)
        self.df_articles = pd.DataFrame(self.articles_data)
        
        # æ•°æ®æ¸…æ´—å’Œè½¬æ¢
        if not self.df_links.empty:
            self.df_links['date'] = pd.to_datetime(self.df_links['date'])
        
        if not self.df_articles.empty:
            self.df_articles['create_time'] = pd.to_datetime(self.df_articles['create_time'])
            self.df_articles['modify_time'] = pd.to_datetime(self.df_articles['modify_time'])
    
    def extract_article_id(self, url):
        """ä»URLä¸­æå–æ–‡ç« ID"""
        patterns = [
            r'B(\d+)\.html',
            r'([a-f0-9-]{36})\.html'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, url)
            if match:
                return match.group(1)
        
        return "unknown"
    
    def extract_date_from_link(self, link):
        """ä»é“¾æ¥ä¸­æå–æ—¥æœŸ"""
        match = re.search(r'B(\d{8})', link)
        if match:
            date_str = match.group(1)
            try:
                return datetime.strptime(date_str, '%Y%m%d').date()
            except:
                pass
        
        return datetime.now().date()
    
    def get_comprehensive_stats(self):
        """è·å–ç»¼åˆç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            'åŸºç¡€ç»Ÿè®¡': self.get_basic_stats(),
            'æ—¶é—´åˆ†æ': self.analyze_by_time(),
            'å…³é”®è¯åˆ†æ': self.analyze_keywords(),
            'æ–‡ä»¶åˆ†æ': self.analyze_files(),
            'è¶‹åŠ¿åˆ†æ': self.analyze_trends()
        }
        
        return stats
    
    def get_basic_stats(self):
        """è·å–åŸºç¡€ç»Ÿè®¡"""
        return {
            'æ€»é“¾æ¥æ•°': len(self.df_links) if self.df_links is not None else 0,
            'å·²ä¸‹è½½æ–‡ç« æ•°': len(self.df_articles) if self.df_articles is not None else 0,
            'ä¸‹è½½ç‡': (len(self.df_articles) / len(self.df_links) * 100) if self.df_links is not None and len(self.df_links) > 0 else 0,
            'æ•°æ®è¦†ç›–å¤©æ•°': self.get_data_coverage_days(),
            'å¹³å‡æ¯æ—¥é“¾æ¥æ•°': self.get_avg_daily_links()
        }
    
    def get_data_coverage_days(self):
        """è·å–æ•°æ®è¦†ç›–å¤©æ•°"""
        if self.df_links is None or self.df_links.empty:
            return 0
        
        date_range = self.df_links['date'].max() - self.df_links['date'].min()
        return date_range.days + 1
    
    def get_avg_daily_links(self):
        """è·å–å¹³å‡æ¯æ—¥é“¾æ¥æ•°"""
        days = self.get_data_coverage_days()
        if days == 0:
            return 0
        
        return len(self.df_links) / days if self.df_links is not None else 0
    
    def analyze_by_time(self):
        """æ—¶é—´åºåˆ—åˆ†æ"""
        if self.df_links is None or self.df_links.empty:
            return {}
        
        # æ—¥åº¦åˆ†æ
        daily_counts = self.df_links.groupby(self.df_links['date'].dt.date).size()
        
        # æœˆåº¦åˆ†æ
        monthly_counts = self.df_links.groupby(self.df_links['date'].dt.to_period('M')).size()
        
        # å¹´åº¦åˆ†æ
        yearly_counts = self.df_links.groupby(self.df_links['date'].dt.year).size()
        
        # æ˜ŸæœŸåˆ†æ
        self.df_links['weekday'] = self.df_links['date'].dt.day_name()
        weekday_counts = self.df_links['weekday'].value_counts()
        
        return {
            'daily_distribution': daily_counts.to_dict(),
            'monthly_distribution': monthly_counts.to_dict(),
            'yearly_distribution': yearly_counts.to_dict(),
            'weekday_distribution': weekday_counts.to_dict(),
            'peak_day': daily_counts.idxmax() if not daily_counts.empty else None,
            'peak_count': daily_counts.max() if not daily_counts.empty else 0
        }
    
    def analyze_keywords(self, top_n=30):
        """å¢å¼ºç‰ˆå…³é”®è¯åˆ†æ"""
        if not self.articles_data:
            return {}
        
        all_titles = " ".join([article['title'] for article in self.articles_data])
        
        # åˆ†è¯
        words = jieba.cut(all_titles)
        
        # è¿‡æ»¤
        stop_words = self.get_stop_words()
        filtered_words = [word for word in words if len(word) > 1 and word not in stop_words]
        
        # è¯é¢‘ç»Ÿè®¡
        word_freq = Counter(filtered_words)
        
        # å…³é”®è¯åˆ†ç±»
        categories = self.categorize_keywords(word_freq.most_common(top_n))
        
        return {
            'top_keywords': word_freq.most_common(top_n),
            'total_words': len(filtered_words),
            'unique_words': len(set(filtered_words)),
            'word_density': len(filtered_words) / len(self.articles_data) if self.articles_data else 0,
            'categories': categories
        }
    
    def get_stop_words(self):
        """è·å–åœç”¨è¯åˆ—è¡¨"""
        basic_stop_words = {
            'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™', 'é‚£', 'äº›', 'ä¸ª'
        }
        
        # å¯ä»¥æ·»åŠ æ›´å¤šåœç”¨è¯
        return basic_stop_words
    
    def categorize_keywords(self, keywords):
        """å…³é”®è¯åˆ†ç±»"""
        categories = {
            'æ”¿ç­–ç›¸å…³': [],
            'è¡Œä¸šç›¸å…³': [],
            'ç»„ç»‡ç›¸å…³': [],
            'æ—¶é—´ç›¸å…³': [],
            'å…¶ä»–': []
        }
        
        policy_words = ['æ”¿ç­–', 'è§„å®š', 'åˆ¶åº¦', 'æ³•è§„', 'æ ‡å‡†', 'è§„èŒƒ', 'é€šçŸ¥', 'å…¬å‘Š', 'æŒ‡å¯¼æ„è§']
        industry_words = ['è¡Œä¸š', 'äº§ä¸š', 'å¸‚åœº', 'ä¼ä¸š', 'å…¬å¸', 'ä¸šåŠ¡', 'æœåŠ¡', 'äº§å“']
        org_words = ['å§”å‘˜ä¼š', 'åä¼š', 'ç»„ç»‡', 'æœºæ„', 'éƒ¨é—¨', 'å•ä½', 'å›¢ä½“']
        time_words = ['å¹´', 'æœˆ', 'æ—¥', 'å­£åº¦', 'å¹´åº¦', 'æœˆåº¦', 'å‘¨æœŸ']
        
        for word, freq in keywords:
            if any(pw in word for pw in policy_words):
                categories['æ”¿ç­–ç›¸å…³'].append((word, freq))
            elif any(iw in word for iw in industry_words):
                categories['è¡Œä¸šç›¸å…³'].append((word, freq))
            elif any(ow in word for ow in org_words):
                categories['ç»„ç»‡ç›¸å…³'].append((word, freq))
            elif any(tw in word for tw in time_words):
                categories['æ—¶é—´ç›¸å…³'].append((word, freq))
            else:
                categories['å…¶ä»–'].append((word, freq))
        
        return categories
    
    def analyze_files(self):
        """æ–‡ä»¶åˆ†æ"""
        if not self.articles_data:
            return {}
        
        file_sizes = [article['file_size'] for article in self.articles_data]
        
        return {
            'æ€»æ–‡ä»¶å¤§å°': sum(file_sizes),
            'å¹³å‡æ–‡ä»¶å¤§å°': sum(file_sizes) / len(file_sizes) if file_sizes else 0,
            'æœ€å¤§æ–‡ä»¶': max(file_sizes) if file_sizes else 0,
            'æœ€å°æ–‡ä»¶': min(file_sizes) if file_sizes else 0,
            'æ–‡ä»¶æ€»æ•°': len(self.articles_data)
        }
    
    def analyze_trends(self):
        """è¶‹åŠ¿åˆ†æ"""
        if self.df_links is None or self.df_links.empty:
            return {}
        
        # è®¡ç®—æ»šåŠ¨å¹³å‡
        daily_counts = self.df_links.groupby(self.df_links['date'].dt.date).size()
        
        # ç®€å•è¶‹åŠ¿åˆ¤æ–­
        if len(daily_counts) > 1:
            recent_avg = daily_counts.tail(7).mean()  # æœ€è¿‘7å¤©å¹³å‡
            overall_avg = daily_counts.mean()  # æ€»ä½“å¹³å‡
            
            trend = "ä¸Šå‡" if recent_avg > overall_avg * 1.1 else \
                   "ä¸‹é™" if recent_avg < overall_avg * 0.9 else "å¹³ç¨³"
        else:
            trend = "æ•°æ®ä¸è¶³"
        
        return {
            'æ•´ä½“è¶‹åŠ¿': trend,
            'æ´»è·ƒåº¦æŒ‡æ•°': self.calculate_activity_index(),
            'æ›´æ–°é¢‘ç‡': self.calculate_update_frequency()
        }
    
    def calculate_activity_index(self):
        """è®¡ç®—æ´»è·ƒåº¦æŒ‡æ•°"""
        if self.df_links is None or self.df_links.empty:
            return 0
        
        # åŸºäºæœ€è¿‘30å¤©çš„æ•°æ®
        recent_data = self.df_links[
            self.df_links['date'] >= datetime.now() - timedelta(days=30)
        ]
        
        return len(recent_data) / 30 * 100  # æ¯æ—¥å¹³å‡é“¾æ¥æ•° * 100
    
    def calculate_update_frequency(self):
        """è®¡ç®—æ›´æ–°é¢‘ç‡"""
        if self.df_links is None or self.df_links.empty:
            return "æ— æ•°æ®"
        
        days = self.get_data_coverage_days()
        total_links = len(self.df_links)
        
        if days == 0:
            return "æ— æ•°æ®"
        
        freq = total_links / days
        
        if freq >= 5:
            return "é«˜é¢‘"
        elif freq >= 2:
            return "ä¸­é¢‘"
        elif freq >= 0.5:
            return "ä½é¢‘"
        else:
            return "æä½é¢‘"
    
    def create_interactive_dashboard(self):
        """åˆ›å»ºäº¤äº’å¼ä»ªè¡¨æ¿"""
        try:
            import streamlit as st
            
            st.set_page_config(
                page_title="æ¯æ—¥ä¿¡æ¯åˆ†æä»ªè¡¨æ¿",
                page_icon="ğŸ“Š",
                layout="wide"
            )
            
            st.title("ğŸ“Š æ¯æ—¥ä¿¡æ¯åˆ†æä»ªè¡¨æ¿")
            
            # åŠ è½½æ•°æ®
            if st.button("é‡æ–°åŠ è½½æ•°æ®"):
                self.load_and_process_data()
                st.success("æ•°æ®å·²é‡æ–°åŠ è½½ï¼")
            
            # ä¾§è¾¹æ 
            with st.sidebar:
                st.header("æ§åˆ¶é¢æ¿")
                analysis_type = st.selectbox(
                    "é€‰æ‹©åˆ†æç±»å‹",
                    ["åŸºç¡€ç»Ÿè®¡", "æ—¶é—´åˆ†æ", "å…³é”®è¯åˆ†æ", "æ–‡ä»¶åˆ†æ", "è¶‹åŠ¿åˆ†æ", "ç»¼åˆæŠ¥å‘Š"]
                )
                
                time_range = st.slider(
                    "æ—¶é—´èŒƒå›´ï¼ˆå¤©ï¼‰",
                    min_value=7,
                    max_value=365,
                    value=30
                )
            
            # ä¸»è¦å†…å®¹åŒºåŸŸ
            if analysis_type == "åŸºç¡€ç»Ÿè®¡":
                self.show_basic_stats()
            elif analysis_type == "æ—¶é—´åˆ†æ":
                self.show_time_analysis()
            elif analysis_type == "å…³é”®è¯åˆ†æ":
                self.show_keyword_analysis()
            elif analysis_type == "æ–‡ä»¶åˆ†æ":
                self.show_file_analysis()
            elif analysis_type == "è¶‹åŠ¿åˆ†æ":
                self.show_trend_analysis()
            elif analysis_type == "ç»¼åˆæŠ¥å‘Š":
                self.show_comprehensive_report()
            
        except ImportError:
            print("Streamlit æœªå®‰è£…ï¼Œæ— æ³•åˆ›å»ºäº¤äº’å¼ä»ªè¡¨æ¿")
            print("è¯·è¿è¡Œ: pip install streamlit")
    
    def show_basic_stats(self):
        """æ˜¾ç¤ºåŸºç¡€ç»Ÿè®¡"""
        st.header("åŸºç¡€ç»Ÿè®¡ä¿¡æ¯")
        
        col1, col2, col3, col4 = st.columns(4)
        
        basic_stats = self.get_basic_stats()
        
        with col1:
            st.metric("æ€»é“¾æ¥æ•°", basic_stats['æ€»é“¾æ¥æ•°'])
        
        with col2:
            st.metric("å·²ä¸‹è½½æ–‡ç« æ•°", basic_stats['å·²ä¸‹è½½æ–‡ç« æ•°'])
        
        with col3:
            st.metric("ä¸‹è½½ç‡", f"{basic_stats['ä¸‹è½½ç‡']:.1f}%")
        
        with col4:
            st.metric("å¹³å‡æ¯æ—¥é“¾æ¥æ•°", f"{basic_stats['å¹³å‡æ¯æ—¥é“¾æ¥æ•°']:.1f}")
    
    def show_time_analysis(self):
        """æ˜¾ç¤ºæ—¶é—´åˆ†æ"""
        st.header("æ—¶é—´åˆ†æ")
        
        time_analysis = self.analyze_by_time()
        
        if time_analysis:
            # æœˆåº¦åˆ†å¸ƒå›¾
            monthly_data = time_analysis['monthly_distribution']
            if monthly_data:
                months = list(monthly_data.keys())
                counts = list(monthly_data.values())
                
                fig = px.bar(x=months, y=counts, title="æœˆåº¦æ–‡ç« å‘å¸ƒåˆ†å¸ƒ")
                fig.update_xaxes(title="æœˆä»½")
                fig.update_yaxes(title="æ–‡ç« æ•°é‡")
                st.plotly_chart(fig, use_container_width=True)
    
    def show_keyword_analysis(self):
        """æ˜¾ç¤ºå…³é”®è¯åˆ†æ"""
        st.header("å…³é”®è¯åˆ†æ")
        
        keyword_analysis = self.analyze_keywords()
        
        if keyword_analysis:
            # è¯äº‘
            keywords = dict(keyword_analysis['top_keywords'][:50])
            
            wordcloud = WordCloud(
                width=800, height=400,
                background_color='white',
                font_path='simhei.ttf',  # ä¸­æ–‡å­—ä½“
                colormap='viridis'
            ).generate_from_frequencies(keywords)
            
            st.image(wordcloud.to_array(), use_column_width=True)
            
            # å…³é”®è¯è¡¨æ ¼
            st.subheader("çƒ­é—¨å…³é”®è¯")
            keyword_df = pd.DataFrame(
                keyword_analysis['top_keywords'][:20],
                columns=['å…³é”®è¯', 'é¢‘æ¬¡']
            )
            st.dataframe(keyword_df)
    
    def show_file_analysis(self):
        """æ˜¾ç¤ºæ–‡ä»¶åˆ†æ"""
        st.header("æ–‡ä»¶åˆ†æ")
        
        file_analysis = self.analyze_files()
        
        if file_analysis:
            col1, col2, col3 = st.columns(3)
            
            with col1:
                st.metric("æ€»æ–‡ä»¶å¤§å°", f"{file_analysis['æ€»æ–‡ä»¶å¤§å°'] / 1024 / 1024:.2f} MB")
            
            with col2:
                st.metric("å¹³å‡æ–‡ä»¶å¤§å°", f"{file_analysis['å¹³å‡æ–‡ä»¶å¤§å°'] / 1024:.1f} KB")
            
            with col3:
                st.metric("æ–‡ä»¶æ€»æ•°", file_analysis['æ–‡ä»¶æ€»æ•°'])
    
    def show_trend_analysis(self):
        """æ˜¾ç¤ºè¶‹åŠ¿åˆ†æ"""
        st.header("è¶‹åŠ¿åˆ†æ")
        
        trend_analysis = self.analyze_trends()
        
        if trend_analysis:
            col1, col2, col3 = st.columns(3)
            
            with col1:
                st.metric("æ•´ä½“è¶‹åŠ¿", trend_analysis['æ•´ä½“è¶‹åŠ¿'])
            
            with col2:
                st.metric("æ´»è·ƒåº¦æŒ‡æ•°", f"{trend_analysis['æ´»è·ƒåº¦æŒ‡æ•°']:.1f}")
            
            with col3:
                st.metric("æ›´æ–°é¢‘ç‡", trend_analysis['æ›´æ–°é¢‘ç‡'])
    
    def show_comprehensive_report(self):
        """æ˜¾ç¤ºç»¼åˆæŠ¥å‘Š"""
        st.header("ç»¼åˆæŠ¥å‘Š")
        
        # è·å–æ‰€æœ‰åˆ†æç»“æœ
        comprehensive_stats = self.get_comprehensive_stats()
        
        # åˆ›å»ºä»ªè¡¨æ¿å¸ƒå±€
        col1, col2 = st.columns(2)
        
        with col1:
            st.subheader("åŸºç¡€ç»Ÿè®¡")
            basic_stats = comprehensive_stats['åŸºç¡€ç»Ÿè®¡']
            for key, value in basic_stats.items():
                st.write(f"**{key}:** {value}")
        
        with col2:
            st.subheader("æ–‡ä»¶ç»Ÿè®¡")
            file_stats = comprehensive_stats['æ–‡ä»¶åˆ†æ']
            for key, value in file_stats.items():
                st.write(f"**{key}:** {value}")
        
        # è¶‹åŠ¿åˆ†æ
        st.subheader("è¶‹åŠ¿åˆ†æ")
        trend_stats = comprehensive_stats['è¶‹åŠ¿åˆ†æ']
        for key, value in trend_stats.items():
            st.write(f"**{key}:** {value}")
    
    def generate_static_report(self, output_dir='reports'):
        """ç”Ÿæˆé™æ€æŠ¥å‘Š"""
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        report_dir = os.path.join(output_dir, f'report_{timestamp}')
        os.makedirs(report_dir)
        
        # è·å–åˆ†æç»“æœ
        comprehensive_stats = self.get_comprehensive_stats()
        
        # ç”ŸæˆHTMLæŠ¥å‘Š
        html_report = self.create_detailed_html_report(comprehensive_stats)
        
        with open(os.path.join(report_dir, 'report.html'), 'w', encoding='utf-8') as f:
            f.write(html_report)
        
        # ç”Ÿæˆå›¾è¡¨
        self.generate_detailed_charts(comprehensive_stats, report_dir)
        
        return report_dir
    
    def create_detailed_html_report(self, stats):
        """åˆ›å»ºè¯¦ç»†çš„HTMLæŠ¥å‘Š"""
        html = f"""
        <!DOCTYPE html>
        <html lang="zh-CN">
        <head>
            <meta charset="UTF-8">
            <meta name="viewport" content="width=device-width, initial-scale=1.0">
            <title>è¯¦ç»†åˆ†ææŠ¥å‘Š - {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥')}</title>
            <style>
                body {{ font-family: 'Microsoft YaHei', Arial, sans-serif; margin: 0; padding: 20px; background-color: #f5f5f5; }}
                .container {{ max-width: 1200px; margin: 0 auto; background-color: white; padding: 30px; border-radius: 10px; box-shadow: 0 0 20px rgba(0,0,0,0.1); }}
                .header {{ text-align: center; margin-bottom: 40px; padding: 30px; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; border-radius: 10px; }}
                .section {{ margin: 30px 0; padding: 25px; border: 1px solid #e0e0e0; border-radius: 8px; background-color: #fafafa; }}
                .stats-grid {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(250px, 1fr)); gap: 20px; margin: 20px 0; }}
                .stat-card {{ background: white; padding: 20px; border-radius: 8px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); text-align: center; transition: transform 0.3s; }}
                .stat-card:hover {{ transform: translateY(-5px); }}
                .stat-number {{ font-size: 2.5em; font-weight: bold; color: #667eea; margin: 10px 0; }}
                .stat-label {{ color: #666; font-size: 1.1em; }}
                .chart-container {{ margin: 20px 0; text-align: center; }}
                .keyword-list {{ display: grid; grid-template-columns: repeat(auto-fill, minmax(200px, 1fr)); gap: 10px; margin: 20px 0; }}
                .keyword-item {{ background: white; padding: 10px; border-radius: 5px; box-shadow: 0 1px 3px rgba(0,0,0,0.1); display: flex; justify-content: space-between; align-items: center; }}
                .footer {{ text-align: center; margin-top: 40px; padding: 20px; background-color: #f0f0f0; border-radius: 8px; color: #666; }}
            </style>
        </head>
        <body>
            <div class="container">
                <div class="header">
                    <h1>ğŸ“Š æ¯æ—¥ä¿¡æ¯è¯¦ç»†åˆ†ææŠ¥å‘Š</h1>
                    <p>ç”Ÿæˆæ—¶é—´ï¼š{datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M')}</p>
                </div>
        """
        
        # åŸºç¡€ç»Ÿè®¡
        basic_stats = stats['åŸºç¡€ç»Ÿè®¡']
        html += f"""
                <div class="section">
                    <h2>ğŸ“ˆ åŸºç¡€ç»Ÿè®¡ä¿¡æ¯</h2>
                    <div class="stats-grid">
                        <div class="stat-card">
                            <div class="stat-number">{basic_stats['æ€»é“¾æ¥æ•°']}</div>
                            <div class="stat-label">æ€»é“¾æ¥æ•°</div>
                        </div>
                        <div class="stat-card">
                            <div class="stat-number">{basic_stats['å·²ä¸‹è½½æ–‡ç« æ•°']}</div>
                            <div class="stat-label">å·²ä¸‹è½½æ–‡ç« æ•°</div>
                        </div>
                        <div class="stat-card">
                            <div class="stat-number">{basic_stats['ä¸‹è½½ç‡']:.1f}%</div>
                            <div class="stat-label">ä¸‹è½½ç‡</div>
                        </div>
                        <div class="stat-card">
                            <div class="stat-number">{basic_stats['æ•°æ®è¦†ç›–å¤©æ•°']}</div>
                            <div class="stat-label">æ•°æ®è¦†ç›–å¤©æ•°</div>
                        </div>
                        <div class="stat-card">
                            <div class="stat-number">{basic_stats['å¹³å‡æ¯æ—¥é“¾æ¥æ•°']:.1f}</div>
                            <div class="stat-label">å¹³å‡æ¯æ—¥é“¾æ¥æ•°</div>
                        </div>
                    </div>
                </div>
        """
        
        # æ–‡ä»¶ç»Ÿè®¡
        file_stats = stats['æ–‡ä»¶åˆ†æ']
        html += f"""
                <div class="section">
                    <h2>ğŸ“ æ–‡ä»¶ç»Ÿè®¡ä¿¡æ¯</h2>
                    <div class="stats-grid">
                        <div class="stat-card">
                            <div class="stat-number">{file_stats['æ€»æ–‡ä»¶å¤§å°'] / 1024 / 1024:.2f} MB</div>
                            <div class="stat-label">æ€»æ–‡ä»¶å¤§å°</div>
                        </div>
                        <div class="stat-card">
                            <div class="stat-number">{file_stats['å¹³å‡æ–‡ä»¶å¤§å°'] / 1024:.1f} KB</div>
                            <div class="stat-label">å¹³å‡æ–‡ä»¶å¤§å°</div>
                        </div>
                        <div class="stat-card">
                            <div class="stat-number">{file_stats['æ–‡ä»¶æ€»æ•°']}</div>
                            <div class="stat-label">æ–‡ä»¶æ€»æ•°</div>
                        </div>
                    </div>
                </div>
        """
        
        # è¶‹åŠ¿åˆ†æ
        trend_stats = stats['è¶‹åŠ¿åˆ†æ']
        html += f"""
                <div class="section">
                    <h2>ğŸ“Š è¶‹åŠ¿åˆ†æ</h2>
                    <div class="stats-grid">
                        <div class="stat-card">
                            <div class="stat-number">{trend_stats['æ•´ä½“è¶‹åŠ¿']}</div>
                            <div class="stat-label">æ•´ä½“è¶‹åŠ¿</div>
                        </div>
                        <div class="stat-card">
                            <div class="stat-number">{trend_stats['æ´»è·ƒåº¦æŒ‡æ•°']:.1f}</div>
                            <div class="stat-label">æ´»è·ƒåº¦æŒ‡æ•°</div>
                        </div>
                        <div class="stat-card">
                            <div class="stat-number">{trend_stats['æ›´æ–°é¢‘ç‡']}</div>
                            <div class="stat-label">æ›´æ–°é¢‘ç‡</div>
                        </div>
                    </div>
                </div>
        """
        
        # å…³é”®è¯åˆ†æ
        keyword_stats = stats['å…³é”®è¯åˆ†æ']
        if keyword_stats:
            html += f"""
                <div class="section">
                    <h2>ğŸ”¤ å…³é”®è¯åˆ†æ</h2>
                    <p>æ€»è¯æ•°ï¼š{keyword_stats['total_words']} | ç‹¬ç‰¹è¯æ•°ï¼š{keyword_stats['unique_words']} | è¯å¯†åº¦ï¼š{keyword_stats['word_density']:.2f}</p>
                    <div class="keyword-list">
            """
            
            for word, freq in keyword_stats['top_keywords'][:30]:
                html += f'<div class="keyword-item"><span>{word}</span><span>{freq}</span></div>'
            
            html += """
                    </div>
                </div>
            """
        
        # æ—¶é—´åˆ†æ
        time_stats = stats['æ—¶é—´åˆ†æ']
        if time_stats:
            html += f"""
                <div class="section">
                    <h2>â° æ—¶é—´åˆ†å¸ƒåˆ†æ</h2>
                    <div class="chart-container">
                        <p><strong>å³°å€¼æ—¥æœŸï¼š</strong> {time_stats['peak_day']} ({time_stats['peak_count']} ç¯‡)</p>
                    </div>
                    <div class="chart-container">
                        <img src="monthly_distribution.png" alt="æœˆåº¦åˆ†å¸ƒå›¾" style="max-width: 100%; height: auto;">
                    </div>
                </div>
            """
        
        html += f"""
                <div class="footer">
                    <p>æŠ¥å‘Šç”Ÿæˆæ—¶é—´ï¼š{datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}</p>
                    <p>æœ¬æŠ¥å‘Šç”±æ¯æ—¥ä¿¡æ¯åˆ†æç¨‹åºè‡ªåŠ¨ç”Ÿæˆ</p>
                </div>
            </div>
        </body>
        </html>
        """
        
        return html
    
    def generate_detailed_charts(self, stats, output_dir):
        """ç”Ÿæˆè¯¦ç»†å›¾è¡¨"""
        time_stats = stats['æ—¶é—´åˆ†æ']
        
        if time_stats and time_stats['monthly_distribution']:
            # æœˆåº¦åˆ†å¸ƒå›¾
            plt.figure(figsize=(15, 8))
            
            monthly_data = time_stats['monthly_distribution']
            months = list(monthly_data.keys())
            counts = list(monthly_data.values())
            
            plt.subplot(2, 1, 1)
            plt.bar(months, counts, color='skyblue', alpha=0.7)
            plt.title('æ–‡ç« æœˆåº¦å‘å¸ƒåˆ†å¸ƒ', fontsize=16, fontweight='bold')
            plt.xlabel('æœˆä»½', fontsize=12)
            plt.ylabel('æ–‡ç« æ•°é‡', fontsize=12)
            plt.xticks(rotation=45)
            plt.grid(axis='y', alpha=0.3)
            
            # æ—¥åº¦åˆ†å¸ƒå›¾ï¼ˆæœ€è¿‘30å¤©ï¼‰
            plt.subplot(2, 1, 2)
            daily_data = time_stats['daily_distribution']
            if daily_data:
                # åªæ˜¾ç¤ºæœ€è¿‘30å¤©
                recent_days = sorted(daily_data.keys())[-30:]
                recent_counts = [daily_data[day] for day in recent_days]
                
                plt.plot(recent_days, recent_counts, marker='o', linewidth=2, markersize=4)
                plt.title('æœ€è¿‘30å¤©æ–‡ç« å‘å¸ƒè¶‹åŠ¿', fontsize=16, fontweight='bold')
                plt.xlabel('æ—¥æœŸ', fontsize=12)
                plt.ylabel('æ–‡ç« æ•°é‡', fontsize=12)
                plt.xticks(rotation=45)
                plt.grid(alpha=0.3)
            
            plt.tight_layout()
            plt.savefig(os.path.join(output_dir, 'monthly_distribution.png'), dpi=300, bbox_inches='tight')
            plt.close()
        
        # å…³é”®è¯è¯äº‘
        keyword_stats = stats['å…³é”®è¯åˆ†æ']
        if keyword_stats:
            keywords = dict(keyword_stats['top_keywords'][:100])
            
            plt.figure(figsize=(12, 8))
            wordcloud = WordCloud(
                width=1200, height=800,
                background_color='white',
                colormap='viridis',
                max_words=100,
                relative_scaling=0.5,
                min_font_size=10
            ).generate_from_frequencies(keywords)
            
            plt.imshow(wordcloud, interpolation='bilinear')
            plt.axis('off')
            plt.title('å…³é”®è¯è¯äº‘', fontsize=20, fontweight='bold', pad=20)
            plt.tight_layout()
            plt.savefig(os.path.join(output_dir, 'keyword_wordcloud.png'), dpi=300, bbox_inches='tight')
            plt.close()

def main():
    """ä¸»å‡½æ•°"""
    print("=== å¢å¼ºç‰ˆæ¯æ—¥ä¿¡æ¯åˆ†æç¨‹åº ===")
    
    # åˆ›å»ºåˆ†æå™¨
    analyzer = EnhancedInfoAnalyzer()
    
    # åŠ è½½æ•°æ®
    print("æ­£åœ¨åŠ è½½æ•°æ®...")
    if not analyzer.load_and_process_data():
        print("æ•°æ®åŠ è½½å¤±è´¥ï¼Œç¨‹åºé€€å‡º")
        return
    
    # ç”Ÿæˆé™æ€æŠ¥å‘Š
    print("æ­£åœ¨ç”Ÿæˆé™æ€æŠ¥å‘Š...")
    report_dir = analyzer.generate_static_report()
    
    print(f"\n=== åˆ†æå®Œæˆ ===")
    print(f"æŠ¥å‘Šç›®å½•ï¼š{report_dir}")
    print(f"æŠ¥å‘Šæ–‡ä»¶ï¼š{os.path.join(report_dir, 'report.html')}")
    print(f"å›¾è¡¨æ–‡ä»¶ï¼š{os.path.join(report_dir, 'monthly_distribution.png')}")
    print(f"è¯äº‘æ–‡ä»¶ï¼š{os.path.join(report_dir, 'keyword_wordcloud.png')}")
    
    # å¯é€‰ï¼šå¯åŠ¨äº¤äº’å¼ä»ªè¡¨æ¿
    print("\næ˜¯å¦å¯åŠ¨äº¤äº’å¼ä»ªè¡¨æ¿ï¼Ÿ(éœ€è¦å®‰è£…streamlit)")
    response = input("è¾“å…¥ 'y' å¯åŠ¨ï¼Œå…¶ä»–é”®é€€å‡º: ")
    
    if response.lower() == 'y':
        try:
            analyzer.create_interactive_dashboard()
        except Exception as e:
            print(f"å¯åŠ¨äº¤äº’å¼ä»ªè¡¨æ¿å¤±è´¥ï¼š{e}")

if __name__ == "__main__":
    main()